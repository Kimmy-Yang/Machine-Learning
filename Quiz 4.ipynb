{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "33f9423b-454a-48a7-bfd3-a1b054b12e90",
   "metadata": {},
   "source": [
    "In this question the behavior of algorithms developed to solve the Support Vector Machine problem are compared. The comparison is carried out on the breast cancer dataset of the sklearn package,\n",
    "\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "The labels of the Breast Cancer dataset are of {0,1} which need to be converted into {âˆ’1,+1}. Scale each of the input variables to have the maximum absolute value equal to 1. For example, these steps can be implemented by\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# load the data\n",
    "X, y = load_breast_cancer(return_X_y=True)  ## X input, y output\n",
    "print(X.shape, y.shape)\n",
    "\n",
    "## to convert the {0,1} output into {-1,+1}\n",
    "y = 2 * y -1\n",
    "\n",
    "## X is the input matrix\n",
    "mdata,ndim = X.shape\n",
    "## normalization by L infinity norm\n",
    "X/= np.outer(np.ones(mdata),np.max(np.abs(X),0))\n",
    "\n",
    "## number of iteration\n",
    "niter = 10\n",
    "\n",
    "## penalty constant for the of the Stochastic Dual Coordinate Ascent algorithm\n",
    "C = 1000\n",
    "Three methods need to be applied. Those methods are the following ones:\n",
    "\n",
    "The Stochastic Dual Coordinate Ascent for SVM algorithm which is described in the lecture slides. In this algorithm, use the linear kernel on the top of the input data. The penalty constant, C, is equal to 1000. Repeat 10 times the algorithm on the full training dataset.\n",
    " The method sklearn.svm.SVC from the sklearn package with polynomial kernel, denoted by poly in the sklearn. The degree of the polynomial is 5, and the penalty constant, C, here is equal to 1, the default.\n",
    "The method sklearn.svm.SVC again from the sklearn package with Gaussian kernel, denoted by rbf in the sklearn. The gamma parameter of the rbf kernel is set to scale, and the penalty constant, C, here is also equal to 1, the default.\n",
    "Process the data in the original order of the examples appearing in the data set. No training and test split is applied. The original full data is used in the training and in the test phases as well.\n",
    "\n",
    "Compute the Area Under the Curve(AUC) score for the three versions of the SVM solution methods, thus for  the Stochastic Dual Coordinate Ascent for SVM, the sklearn.svm.SVC(kernel = \"poly\") and the sklearn.svm.SVC(kernel = \"rbf\"). Let the short names  of the methods be SDCA, SVCPoly, SVCRbf.\n",
    "\n",
    "Question: What is the order of the performances of the methods measured by Area Under the Curve(AUC)?\n",
    "\n",
    "Hint: You might apply the sklearn.metrics.roc_auc_score function to compute the AUC score. The sklearn also contains other alternative approaches."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4470d352-624d-4922-a602-da05fc582fc1",
   "metadata": {},
   "source": [
    "SDCA < SVCRbf < SVCPoly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e3e5e897-20d4-4f17-8b15-2be264150b83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(569, 30) (569,)\n"
     ]
    }
   ],
   "source": [
    "## ####################################################\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import roc_auc_score, f1_score, auc, roc_curve \n",
    "\n",
    "# load the data\n",
    "X, y = load_breast_cancer(return_X_y=True)  ## X input, y output\n",
    "print(X.shape, y.shape)\n",
    "\n",
    "## to convert the {0,1} output into {-1,+1}\n",
    "y = 2 * y -1\n",
    "\n",
    "## X is the input matrix\n",
    "mdata,ndim = X.shape\n",
    "# ## normalization by L infinity norm\n",
    "# X/= np.outer(np.ones(mdata),np.max(np.abs(X),0))\n",
    "\n",
    "iscale = 1 ## =0, no scaling, =1 scaling the by the max absolute value\n",
    "if iscale == 1: ## L_infty norm\n",
    "    X /= np.outer(np.ones(mdata),np.max(np.abs(X),0))\n",
    "elif iscale == 2: ## L2 norm\n",
    "    xnorm = np.sqrt(np.sum(X**2,1))\n",
    "    xnorm = xnorm + (xnorm==0)\n",
    "    X /= np.outer(xnorm, np.ones(ndim))\n",
    "\n",
    "## number of iteration \n",
    "niter = 10  \n",
    "\n",
    "## penalty constant for the of the Stochastic Dual Coordinate Ascent algorithm\n",
    "C = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bbef3adc-663f-4ca0-8ee1-f380a530e069",
   "metadata": {},
   "outputs": [],
   "source": [
    "## dual stochastic gradient algorithm\n",
    "## initialize alpha\n",
    "alpha = np.zeros(mdata)\n",
    "## compute the linear kernel\n",
    "K = np.dot(X,X.T)\n",
    "\n",
    "for iter in range(niter):\n",
    "    for i in range(mdata):\n",
    "        ## process all sample examples sequentiallly\n",
    "        ## sum on i!=j  => sum on all i-(i=j)\n",
    "        alpha[i]=(1-y[i]*(np.sum(K[i]*y*alpha)-alpha[i]*y[i]*K[i,i]))/K[i,i]\n",
    "        ## projection\n",
    "        alpha[i]= min(C/mdata, max(0,alpha[i]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "52beef0c-6d4b-41df-bacf-544cf896fcb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "## accuracy of the full data\n",
    "Xtest =X\n",
    "mtest = X.shape[0]\n",
    "## Linear kernel\n",
    "Kcross = np.dot(X, Xtest.T)\n",
    "ysdca = np.sign(np.sum(np.outer(y*alpha, np.ones(mtest))*Kcross, 0))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1f6b7705-c19f-442b-a60d-7bd3e70dd5bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9382366154008773"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sdca_f1 = f1_score(y, ysdca)\n",
    "sdca_auc = roc_auc_score(y,ysdca)\n",
    "sdca_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6a395958-9cc8-49c1-a0a3-69d76fea23f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9929245283018867"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2. sklearn.svm.SVC poly,SVCPoly\n",
    "clf2 = SVC(C=1, kernel='poly',degree=5)\n",
    "clf2.fit(X,y)\n",
    "y_pred = clf2.predict(X)\n",
    "\n",
    "# roc_auc_score(y, clf.predict_proba(X)[:, 1])\n",
    "roc_auc_score(y, clf2.predict(X))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "100c5290-311d-4211-b574-bf3689a01adf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9773730246815707"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3. Gaussian kernel SVCRbf\n",
    "clf3 = SVC(C=1, kernel='rbf',gamma='scale')\n",
    "clf3.fit(X,y)\n",
    "y_pred = clf3.predict(X)\n",
    "\n",
    "# roc_auc_score(y, clf.predict_proba(X)[:, 1])\n",
    "roc_auc_score(y, clf3.predict(X))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
